{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started. Once you have completed the code you can download the notebook for making a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data path: /home/datasets/Project_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/home/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/home/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [2,4,6,8,10,12,14,16,18,20,22,24,26]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t) // batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,13,80,80,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    image = resize(image,(80,80)).astype(np.float32)\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if len(t) % batch_size != 0:\n",
    "            batch_data = np.zeros((batch_size,13,80,80,3)) \n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = resize(image,(80,80)).astype(np.float32)\n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1 \n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield does  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/home/datasets/Project_data/train'\n",
    "val_path = '/home/datasets/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, Flatten, BatchNormalization, Activation, Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, TimeDistributed, LSTM, Dense, Flatten, Dropout,GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, TimeDistributed, GRU, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Model-Conv3D simple network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your model here\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (3, 3, 3), padding='same',\n",
    "         input_shape=(13,80,80,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_12 (Conv3D)          (None, 13, 80, 80, 32)    2624      \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 13, 80, 80, 32)    0         \n",
      "                                                                 \n",
      " max_pooling3d_12 (MaxPoolin  (None, 6, 40, 40, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_13 (Conv3D)          (None, 6, 40, 40, 64)     16448     \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 6, 40, 40, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_13 (MaxPoolin  (None, 3, 20, 20, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 76800)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                2457632   \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,476,869\n",
      "Trainable params: 2,476,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'adam' #write your optimizer''\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',save_freq = 'epoch')\n",
    "\n",
    "LR =ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit` method to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 17:08:32.979381: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - ETA: 0s - loss: 2.2198 - categorical_accuracy: 0.2039Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_init_2025-03-0317_07_55.516965/model-00001-2.21980-0.20387-1.57967-0.20312.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 2.2198 - categorical_accuracy: 0.2039 - val_loss: 1.5797 - val_categorical_accuracy: 0.2031 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5469 - categorical_accuracy: 0.2411\n",
      "Epoch 00002: saving model to model_init_2025-03-0317_07_55.516965/model-00002-1.54695-0.24107-1.50708-0.28906.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 1.5469 - categorical_accuracy: 0.2411 - val_loss: 1.5071 - val_categorical_accuracy: 0.2891 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4447 - categorical_accuracy: 0.2872\n",
      "Epoch 00003: saving model to model_init_2025-03-0317_07_55.516965/model-00003-1.44469-0.28720-1.29351-0.36719.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 1.4447 - categorical_accuracy: 0.2872 - val_loss: 1.2935 - val_categorical_accuracy: 0.3672 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3583 - categorical_accuracy: 0.3393\n",
      "Epoch 00004: saving model to model_init_2025-03-0317_07_55.516965/model-00004-1.35834-0.33929-1.32331-0.34375.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 1.3583 - categorical_accuracy: 0.3393 - val_loss: 1.3233 - val_categorical_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2631 - categorical_accuracy: 0.3512\n",
      "Epoch 00005: saving model to model_init_2025-03-0317_07_55.516965/model-00005-1.26310-0.35119-1.26534-0.35156.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 1.2631 - categorical_accuracy: 0.3512 - val_loss: 1.2653 - val_categorical_accuracy: 0.3516 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1911 - categorical_accuracy: 0.4048\n",
      "Epoch 00006: saving model to model_init_2025-03-0317_07_55.516965/model-00006-1.19114-0.40476-1.25674-0.45312.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 1.1911 - categorical_accuracy: 0.4048 - val_loss: 1.2567 - val_categorical_accuracy: 0.4531 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0924 - categorical_accuracy: 0.4762\n",
      "Epoch 00007: saving model to model_init_2025-03-0317_07_55.516965/model-00007-1.09243-0.47619-1.17562-0.37500.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 1.0924 - categorical_accuracy: 0.4762 - val_loss: 1.1756 - val_categorical_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0182 - categorical_accuracy: 0.4836\n",
      "Epoch 00008: saving model to model_init_2025-03-0317_07_55.516965/model-00008-1.01823-0.48363-1.12114-0.47656.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 1.0182 - categorical_accuracy: 0.4836 - val_loss: 1.1211 - val_categorical_accuracy: 0.4766 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9443 - categorical_accuracy: 0.5030\n",
      "Epoch 00009: saving model to model_init_2025-03-0317_07_55.516965/model-00009-0.94425-0.50298-1.11273-0.43750.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.9443 - categorical_accuracy: 0.5030 - val_loss: 1.1127 - val_categorical_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8798 - categorical_accuracy: 0.5342\n",
      "Epoch 00010: saving model to model_init_2025-03-0317_07_55.516965/model-00010-0.87983-0.53423-1.10785-0.42969.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.8798 - categorical_accuracy: 0.5342 - val_loss: 1.1079 - val_categorical_accuracy: 0.4297 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8399 - categorical_accuracy: 0.5491\n",
      "Epoch 00011: saving model to model_init_2025-03-0317_07_55.516965/model-00011-0.83992-0.54911-1.30592-0.43750.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.8399 - categorical_accuracy: 0.5491 - val_loss: 1.3059 - val_categorical_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8599 - categorical_accuracy: 0.5551\n",
      "Epoch 00012: saving model to model_init_2025-03-0317_07_55.516965/model-00012-0.85986-0.55506-1.07747-0.49219.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.8599 - categorical_accuracy: 0.5551 - val_loss: 1.0775 - val_categorical_accuracy: 0.4922 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7483 - categorical_accuracy: 0.5938\n",
      "Epoch 00013: saving model to model_init_2025-03-0317_07_55.516965/model-00013-0.74830-0.59375-1.12176-0.57031.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.7483 - categorical_accuracy: 0.5938 - val_loss: 1.1218 - val_categorical_accuracy: 0.5703 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7157 - categorical_accuracy: 0.5938\n",
      "Epoch 00014: saving model to model_init_2025-03-0317_07_55.516965/model-00014-0.71568-0.59375-1.13287-0.54688.h5\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.7157 - categorical_accuracy: 0.5938 - val_loss: 1.1329 - val_categorical_accuracy: 0.5469 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7270 - categorical_accuracy: 0.6295\n",
      "Epoch 00015: saving model to model_init_2025-03-0317_07_55.516965/model-00015-0.72702-0.62946-1.56970-0.52344.h5\n",
      "21/21 [==============================] - 79s 4s/step - loss: 0.7270 - categorical_accuracy: 0.6295 - val_loss: 1.5697 - val_categorical_accuracy: 0.5234 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6789 - categorical_accuracy: 0.6458\n",
      "Epoch 00016: saving model to model_init_2025-03-0317_07_55.516965/model-00016-0.67889-0.64583-1.15708-0.56250.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.6789 - categorical_accuracy: 0.6458 - val_loss: 1.1571 - val_categorical_accuracy: 0.5625 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6274 - categorical_accuracy: 0.6622\n",
      "Epoch 00017: saving model to model_init_2025-03-0317_07_55.516965/model-00017-0.62737-0.66220-1.22058-0.59375.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 0.6274 - categorical_accuracy: 0.6622 - val_loss: 1.2206 - val_categorical_accuracy: 0.5938 - lr: 2.0000e-04\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6095 - categorical_accuracy: 0.6696\n",
      "Epoch 00018: saving model to model_init_2025-03-0317_07_55.516965/model-00018-0.60953-0.66964-1.11410-0.66406.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.6095 - categorical_accuracy: 0.6696 - val_loss: 1.1141 - val_categorical_accuracy: 0.6641 - lr: 2.0000e-04\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5976 - categorical_accuracy: 0.6964\n",
      "Epoch 00019: saving model to model_init_2025-03-0317_07_55.516965/model-00019-0.59765-0.69643-1.30233-0.59375.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.5976 - categorical_accuracy: 0.6964 - val_loss: 1.3023 - val_categorical_accuracy: 0.5938 - lr: 2.0000e-04\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5859 - categorical_accuracy: 0.6920\n",
      "Epoch 00020: saving model to model_init_2025-03-0317_07_55.516965/model-00020-0.58591-0.69196-1.19753-0.55469.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "21/21 [==============================] - 78s 4s/step - loss: 0.5859 - categorical_accuracy: 0.6920 - val_loss: 1.1975 - val_categorical_accuracy: 0.5547 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting: The difference between training and validation accuracy tends to increase as the epochs progress (e.g., from Epoch 10 onwards). This suggests that the model may be overfitting to the training data, as it performs significantly better on the training set compared to the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Model-Conv3D\n",
    "Adding Dropouts at dense layer , adding another Conv3D layer to see if any increase in the Train and Validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 07:09:07.551002: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2025-03-04 07:09:07.551079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14800 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:1c:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (3, 3, 3), padding='same',\n",
    "         input_shape=(13,80,80,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 13, 80, 80, 32)    2624      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 13, 80, 80, 32)    0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 6, 40, 40, 32)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 6, 40, 40, 64)     16448     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6, 40, 40, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 3, 20, 20, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 3, 20, 20, 128)    65664     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3, 20, 20, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 1, 10, 10, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1638528   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,731,845\n",
      "Trainable params: 1,731,845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'adam' #write your optimizer''\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',save_freq = 'epoch')\n",
    "\n",
    "LR =ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6284 - categorical_accuracy: 0.1875Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_init_2025-03-0317_07_55.516965/model-00001-1.62840-0.18750-1.59943-0.24219.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 1.6284 - categorical_accuracy: 0.1875 - val_loss: 1.5994 - val_categorical_accuracy: 0.2422 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5903 - categorical_accuracy: 0.2292\n",
      "Epoch 00002: saving model to model_init_2025-03-0317_07_55.516965/model-00002-1.59026-0.22917-1.53570-0.24219.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 1.5903 - categorical_accuracy: 0.2292 - val_loss: 1.5357 - val_categorical_accuracy: 0.2422 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5228 - categorical_accuracy: 0.2708\n",
      "Epoch 00003: saving model to model_init_2025-03-0317_07_55.516965/model-00003-1.52280-0.27083-1.34918-0.30469.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 1.5228 - categorical_accuracy: 0.2708 - val_loss: 1.3492 - val_categorical_accuracy: 0.3047 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4419 - categorical_accuracy: 0.3557\n",
      "Epoch 00004: saving model to model_init_2025-03-0317_07_55.516965/model-00004-1.44189-0.35565-1.43476-0.31250.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 1.4419 - categorical_accuracy: 0.3557 - val_loss: 1.4348 - val_categorical_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3824 - categorical_accuracy: 0.3899\n",
      "Epoch 00005: saving model to model_init_2025-03-0317_07_55.516965/model-00005-1.38239-0.38988-1.19910-0.44531.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 1.3824 - categorical_accuracy: 0.3899 - val_loss: 1.1991 - val_categorical_accuracy: 0.4453 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2783 - categorical_accuracy: 0.4092\n",
      "Epoch 00006: saving model to model_init_2025-03-0317_07_55.516965/model-00006-1.27831-0.40923-1.22349-0.46875.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 1.2783 - categorical_accuracy: 0.4092 - val_loss: 1.2235 - val_categorical_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2552 - categorical_accuracy: 0.4196\n",
      "Epoch 00007: saving model to model_init_2025-03-0317_07_55.516965/model-00007-1.25523-0.41964-1.12269-0.59375.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 1.2552 - categorical_accuracy: 0.4196 - val_loss: 1.1227 - val_categorical_accuracy: 0.5938 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1711 - categorical_accuracy: 0.4509\n",
      "Epoch 00008: saving model to model_init_2025-03-0317_07_55.516965/model-00008-1.17108-0.45089-1.02820-0.55469.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 1.1711 - categorical_accuracy: 0.4509 - val_loss: 1.0282 - val_categorical_accuracy: 0.5547 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0772 - categorical_accuracy: 0.4940\n",
      "Epoch 00009: saving model to model_init_2025-03-0317_07_55.516965/model-00009-1.07721-0.49405-1.05089-0.59375.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 1.0772 - categorical_accuracy: 0.4940 - val_loss: 1.0509 - val_categorical_accuracy: 0.5938 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0502 - categorical_accuracy: 0.5298\n",
      "Epoch 00010: saving model to model_init_2025-03-0317_07_55.516965/model-00010-1.05019-0.52976-1.01665-0.60156.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 1.0502 - categorical_accuracy: 0.5298 - val_loss: 1.0167 - val_categorical_accuracy: 0.6016 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9763 - categorical_accuracy: 0.5774\n",
      "Epoch 00011: saving model to model_init_2025-03-0317_07_55.516965/model-00011-0.97634-0.57738-0.96893-0.60938.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.9763 - categorical_accuracy: 0.5774 - val_loss: 0.9689 - val_categorical_accuracy: 0.6094 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9826 - categorical_accuracy: 0.5625\n",
      "Epoch 00012: saving model to model_init_2025-03-0317_07_55.516965/model-00012-0.98264-0.56250-0.97288-0.57812.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.9826 - categorical_accuracy: 0.5625 - val_loss: 0.9729 - val_categorical_accuracy: 0.5781 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8920 - categorical_accuracy: 0.6131\n",
      "Epoch 00013: saving model to model_init_2025-03-0317_07_55.516965/model-00013-0.89202-0.61310-0.84324-0.71094.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 0.8920 - categorical_accuracy: 0.6131 - val_loss: 0.8432 - val_categorical_accuracy: 0.7109 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7594 - categorical_accuracy: 0.6786\n",
      "Epoch 00014: saving model to model_init_2025-03-0317_07_55.516965/model-00014-0.75939-0.67857-0.79338-0.64062.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.7594 - categorical_accuracy: 0.6786 - val_loss: 0.7934 - val_categorical_accuracy: 0.6406 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6825 - categorical_accuracy: 0.7202\n",
      "Epoch 00015: saving model to model_init_2025-03-0317_07_55.516965/model-00015-0.68252-0.72024-0.71070-0.72656.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.6825 - categorical_accuracy: 0.7202 - val_loss: 0.7107 - val_categorical_accuracy: 0.7266 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6911 - categorical_accuracy: 0.7068\n",
      "Epoch 00016: saving model to model_init_2025-03-0317_07_55.516965/model-00016-0.69106-0.70685-0.71033-0.76562.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.6911 - categorical_accuracy: 0.7068 - val_loss: 0.7103 - val_categorical_accuracy: 0.7656 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5599 - categorical_accuracy: 0.7738\n",
      "Epoch 00017: saving model to model_init_2025-03-0317_07_55.516965/model-00017-0.55989-0.77381-0.75110-0.67969.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.5599 - categorical_accuracy: 0.7738 - val_loss: 0.7511 - val_categorical_accuracy: 0.6797 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5382 - categorical_accuracy: 0.7723\n",
      "Epoch 00018: saving model to model_init_2025-03-0317_07_55.516965/model-00018-0.53825-0.77232-0.91850-0.70312.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.5382 - categorical_accuracy: 0.7723 - val_loss: 0.9185 - val_categorical_accuracy: 0.7031 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4963 - categorical_accuracy: 0.8289\n",
      "Epoch 00019: saving model to model_init_2025-03-0317_07_55.516965/model-00019-0.49633-0.82887-0.64827-0.78125.h5\n",
      "21/21 [==============================] - 70s 4s/step - loss: 0.4963 - categorical_accuracy: 0.8289 - val_loss: 0.6483 - val_categorical_accuracy: 0.7812 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4402 - categorical_accuracy: 0.8348\n",
      "Epoch 00020: saving model to model_init_2025-03-0317_07_55.516965/model-00020-0.44023-0.83482-0.70808-0.72656.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.4402 - categorical_accuracy: 0.8348 - val_loss: 0.7081 - val_categorical_accuracy: 0.7266 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history2=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "Overfitting:\n",
    "\n",
    "The difference between training and validation accuracy is relatively small in the early epochs but grows larger in later epochs (e.g., Epoch 20 has a difference of 0.1082).\n",
    "\n",
    "This suggests that the model may be starting to overfit to the training data, as it performs significantly better on the training set compared to the validation set in later epochs.\n",
    "\n",
    "Validation Accuracy Fluctuations:\n",
    "Validation accuracy fluctuates slightly across epochs (e.g., drops at Epoch 17 and Epoch 20), which could indicate instability in the model's generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third Model-Conv3D\n",
    "Adding more epochs may give the stable validation accuracy. Before doing that we would try what effect batch_size of 64 have on the model. For this model we will try batch_size 64. Used optimiser \"SGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_3 (Conv3D)           (None, 13, 80, 80, 32)    2624      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 13, 80, 80, 32)    0         \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 6, 40, 40, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 6, 40, 40, 64)     16448     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 6, 40, 40, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 3, 20, 20, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 3, 20, 20, 128)    65664     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 3, 20, 20, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 1, 10, 10, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               1638528   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,731,845\n",
      "Trainable params: 1,731,845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "new_batch_size = 64\n",
    "train_generator = generator(train_path, train_doc, new_batch_size)\n",
    "val_generator = generator(val_path, val_doc, new_batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "optimiser = 'sgd'\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_203/2947920635.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history3 = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs2, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 64\n",
      "Epoch 1/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6123 - categorical_accuracy: 0.1987Source path =  /home/datasets/Project_data/val ; batch size = 64\n",
      "\n",
      "Epoch 00001: saving model to model_init_2025-03-0404_25_08.943004/model-00001-1.61226-0.19866-1.60078-0.23438.h5\n",
      "21/21 [==============================] - 146s 7s/step - loss: 1.6123 - categorical_accuracy: 0.1987 - val_loss: 1.6008 - val_categorical_accuracy: 0.2344 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6055 - categorical_accuracy: 0.2113\n",
      "Epoch 00002: saving model to model_init_2025-03-0404_25_08.943004/model-00002-1.60552-0.21131-1.59640-0.26172.h5\n",
      "21/21 [==============================] - 148s 7s/step - loss: 1.6055 - categorical_accuracy: 0.2113 - val_loss: 1.5964 - val_categorical_accuracy: 0.2617 - lr: 0.0100\n",
      "Epoch 3/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5996 - categorical_accuracy: 0.2440\n",
      "Epoch 00003: saving model to model_init_2025-03-0404_25_08.943004/model-00003-1.59958-0.24405-1.59530-0.21875.h5\n",
      "21/21 [==============================] - 149s 7s/step - loss: 1.5996 - categorical_accuracy: 0.2440 - val_loss: 1.5953 - val_categorical_accuracy: 0.2188 - lr: 0.0100\n",
      "Epoch 4/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5956 - categorical_accuracy: 0.2381\n",
      "Epoch 00004: saving model to model_init_2025-03-0404_25_08.943004/model-00004-1.59560-0.23810-1.58146-0.24609.h5\n",
      "21/21 [==============================] - 153s 8s/step - loss: 1.5956 - categorical_accuracy: 0.2381 - val_loss: 1.5815 - val_categorical_accuracy: 0.2461 - lr: 0.0100\n",
      "Epoch 5/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5917 - categorical_accuracy: 0.2545\n",
      "Epoch 00005: saving model to model_init_2025-03-0404_25_08.943004/model-00005-1.59174-0.25446-1.57707-0.34375.h5\n",
      "21/21 [==============================] - 146s 7s/step - loss: 1.5917 - categorical_accuracy: 0.2545 - val_loss: 1.5771 - val_categorical_accuracy: 0.3438 - lr: 0.0100\n",
      "Epoch 6/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5769 - categorical_accuracy: 0.2961\n",
      "Epoch 00006: saving model to model_init_2025-03-0404_25_08.943004/model-00006-1.57688-0.29613-1.56395-0.29688.h5\n",
      "21/21 [==============================] - 148s 7s/step - loss: 1.5769 - categorical_accuracy: 0.2961 - val_loss: 1.5640 - val_categorical_accuracy: 0.2969 - lr: 0.0100\n",
      "Epoch 7/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5735 - categorical_accuracy: 0.2746\n",
      "Epoch 00007: saving model to model_init_2025-03-0404_25_08.943004/model-00007-1.57351-0.27455-1.54901-0.27344.h5\n",
      "21/21 [==============================] - 149s 7s/step - loss: 1.5735 - categorical_accuracy: 0.2746 - val_loss: 1.5490 - val_categorical_accuracy: 0.2734 - lr: 0.0100\n",
      "Epoch 8/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5630 - categorical_accuracy: 0.2917\n",
      "Epoch 00008: saving model to model_init_2025-03-0404_25_08.943004/model-00008-1.56303-0.29167-1.54030-0.39844.h5\n",
      "21/21 [==============================] - 150s 8s/step - loss: 1.5630 - categorical_accuracy: 0.2917 - val_loss: 1.5403 - val_categorical_accuracy: 0.3984 - lr: 0.0100\n",
      "Epoch 9/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5390 - categorical_accuracy: 0.3185\n",
      "Epoch 00009: saving model to model_init_2025-03-0404_25_08.943004/model-00009-1.53901-0.31845-1.52463-0.32031.h5\n",
      "21/21 [==============================] - 151s 8s/step - loss: 1.5390 - categorical_accuracy: 0.3185 - val_loss: 1.5246 - val_categorical_accuracy: 0.3203 - lr: 0.0100\n",
      "Epoch 10/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5493 - categorical_accuracy: 0.3162\n",
      "Epoch 00010: saving model to model_init_2025-03-0404_25_08.943004/model-00010-1.54932-0.31622-1.52936-0.46094.h5\n",
      "21/21 [==============================] - 150s 7s/step - loss: 1.5493 - categorical_accuracy: 0.3162 - val_loss: 1.5294 - val_categorical_accuracy: 0.4609 - lr: 0.0100\n",
      "Epoch 11/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5273 - categorical_accuracy: 0.3393\n",
      "Epoch 00011: saving model to model_init_2025-03-0404_25_08.943004/model-00011-1.52726-0.33929-1.49484-0.40625.h5\n",
      "21/21 [==============================] - 149s 7s/step - loss: 1.5273 - categorical_accuracy: 0.3393 - val_loss: 1.4948 - val_categorical_accuracy: 0.4062 - lr: 0.0100\n",
      "Epoch 12/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5107 - categorical_accuracy: 0.3363\n",
      "Epoch 00012: saving model to model_init_2025-03-0404_25_08.943004/model-00012-1.51070-0.33631-1.47798-0.43750.h5\n",
      "21/21 [==============================] - 157s 8s/step - loss: 1.5107 - categorical_accuracy: 0.3363 - val_loss: 1.4780 - val_categorical_accuracy: 0.4375 - lr: 0.0100\n",
      "Epoch 13/30\n",
      " 2/21 [=>............................] - ETA: 1:59 - loss: 1.4929 - categorical_accuracy: 0.4453\n",
      "Epoch 00019: saving model to model_init_2025-03-0404_25_08.943004/model-00019-1.33971-0.43527-1.25502-0.54688.h5\n",
      "21/21 [==============================] - 151s 8s/step - loss: 1.3397 - categorical_accuracy: 0.4353 - val_loss: 1.2550 - val_categorical_accuracy: 0.5469 - lr: 0.0100\n",
      "Epoch 20/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3468 - categorical_accuracy: 0.4338\n",
      "Epoch 00020: saving model to model_init_2025-03-0404_25_08.943004/model-00020-1.34683-0.43378-1.39768-0.38672.h5\n",
      "21/21 [==============================] - 151s 8s/step - loss: 1.3468 - categorical_accuracy: 0.4338 - val_loss: 1.3977 - val_categorical_accuracy: 0.3867 - lr: 0.0100\n",
      "Epoch 21/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3242 - categorical_accuracy: 0.4591\n",
      "Epoch 00021: saving model to model_init_2025-03-0404_25_08.943004/model-00021-1.32418-0.45908-1.21889-0.54688.h5\n",
      "21/21 [==============================] - 148s 7s/step - loss: 1.3242 - categorical_accuracy: 0.4591 - val_loss: 1.2189 - val_categorical_accuracy: 0.5469 - lr: 0.0100\n",
      "Epoch 22/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2663 - categorical_accuracy: 0.4680\n",
      "Epoch 00022: saving model to model_init_2025-03-0404_25_08.943004/model-00022-1.26632-0.46801-1.11289-0.59375.h5\n",
      "21/21 [==============================] - 147s 7s/step - loss: 1.2663 - categorical_accuracy: 0.4680 - val_loss: 1.1129 - val_categorical_accuracy: 0.5938 - lr: 0.0100\n",
      "Epoch 23/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2618 - categorical_accuracy: 0.4792\n",
      "Epoch 00023: saving model to model_init_2025-03-0404_25_08.943004/model-00023-1.26184-0.47917-1.33037-0.40625.h5\n",
      "21/21 [==============================] - 154s 8s/step - loss: 1.2618 - categorical_accuracy: 0.4792 - val_loss: 1.3304 - val_categorical_accuracy: 0.4062 - lr: 0.0100\n",
      "Epoch 24/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1046 - categorical_accuracy: 0.5603\n",
      "Epoch 00030: saving model to model_init_2025-03-0404_25_08.943004/model-00030-1.10459-0.56027-1.06941-0.56250.h5\n",
      "21/21 [==============================] - 147s 7s/step - loss: 1.1046 - categorical_accuracy: 0.5603 - val_loss: 1.0694 - val_categorical_accuracy: 0.5625 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "num_epochs2=30\n",
    "history3 = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs2, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 Summary:\n",
    "The model shows gradual improvement in both training and validation accuracy, but the performance is inconsistent, and convergence is slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth Model-Conv3D\n",
    " We will now experiment in increasing the epochs and reducing the batch size from 64 to 32 to see if any increase in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 13, 80, 80, 32)    2624      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 13, 80, 80, 32)    0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 6, 40, 40, 32)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 6, 40, 40, 64)     16448     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6, 40, 40, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 3, 20, 20, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 3, 20, 20, 128)    65664     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3, 20, 20, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 1, 10, 10, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1638528   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,731,845\n",
      "Trainable params: 1,731,845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "new_batch_size = 32\n",
    "train_generator = generator(train_path, train_doc, new_batch_size)\n",
    "val_generator = generator(val_path, val_doc, new_batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "optimiser = 'adam'\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "improvement in both training and validation accuracy, but signs of overfitting emerge in later epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 07:11:06.946001: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - ETA: 0s - loss: 1.6299 - categorical_accuracy: 0.2173Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_init_2025-03-0407_08_44.828734/model-00001-1.62987-0.21726-1.59869-0.21094.h5\n",
      "21/21 [==============================] - 81s 4s/step - loss: 1.6299 - categorical_accuracy: 0.2173 - val_loss: 1.5987 - val_categorical_accuracy: 0.2109 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5841 - categorical_accuracy: 0.2098\n",
      "Epoch 00002: saving model to model_init_2025-03-0407_08_44.828734/model-00002-1.58407-0.20982-1.52218-0.40625.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 1.5841 - categorical_accuracy: 0.2098 - val_loss: 1.5222 - val_categorical_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5184 - categorical_accuracy: 0.3467\n",
      "Epoch 00003: saving model to model_init_2025-03-0407_08_44.828734/model-00003-1.51845-0.34673-1.34073-0.38281.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 1.5184 - categorical_accuracy: 0.3467 - val_loss: 1.3407 - val_categorical_accuracy: 0.3828 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4168 - categorical_accuracy: 0.3854\n",
      "Epoch 00004: saving model to model_init_2025-03-0407_08_44.828734/model-00004-1.41685-0.38542-1.27846-0.54688.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 1.4168 - categorical_accuracy: 0.3854 - val_loss: 1.2785 - val_categorical_accuracy: 0.5469 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3619 - categorical_accuracy: 0.3988\n",
      "Epoch 00005: saving model to model_init_2025-03-0407_08_44.828734/model-00005-1.36188-0.39881-1.22652-0.40625.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 1.3619 - categorical_accuracy: 0.3988 - val_loss: 1.2265 - val_categorical_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2649 - categorical_accuracy: 0.4182\n",
      "Epoch 00006: saving model to model_init_2025-03-0407_08_44.828734/model-00006-1.26486-0.41815-1.08827-0.61719.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 1.2649 - categorical_accuracy: 0.4182 - val_loss: 1.0883 - val_categorical_accuracy: 0.6172 - lr: 0.0010\n",
      "Epoch 7/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1407 - categorical_accuracy: 0.4717\n",
      "Epoch 00007: saving model to model_init_2025-03-0407_08_44.828734/model-00007-1.14073-0.47173-0.94265-0.52344.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 1.1407 - categorical_accuracy: 0.4717 - val_loss: 0.9426 - val_categorical_accuracy: 0.5234 - lr: 0.0010\n",
      "Epoch 8/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0750 - categorical_accuracy: 0.5074\n",
      "Epoch 00008: saving model to model_init_2025-03-0407_08_44.828734/model-00008-1.07503-0.50744-0.99756-0.60156.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 1.0750 - categorical_accuracy: 0.5074 - val_loss: 0.9976 - val_categorical_accuracy: 0.6016 - lr: 0.0010\n",
      "Epoch 9/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0253 - categorical_accuracy: 0.5446\n",
      "Epoch 00009: saving model to model_init_2025-03-0407_08_44.828734/model-00009-1.02532-0.54464-0.93520-0.62500.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 1.0253 - categorical_accuracy: 0.5446 - val_loss: 0.9352 - val_categorical_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 10/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9389 - categorical_accuracy: 0.5848\n",
      "Epoch 00010: saving model to model_init_2025-03-0407_08_44.828734/model-00010-0.93891-0.58482-0.84247-0.69531.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 0.9389 - categorical_accuracy: 0.5848 - val_loss: 0.8425 - val_categorical_accuracy: 0.6953 - lr: 0.0010\n",
      "Epoch 11/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8326 - categorical_accuracy: 0.6399\n",
      "Epoch 00011: saving model to model_init_2025-03-0407_08_44.828734/model-00011-0.83262-0.63988-0.61981-0.71094.h5\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.8326 - categorical_accuracy: 0.6399 - val_loss: 0.6198 - val_categorical_accuracy: 0.7109 - lr: 0.0010\n",
      "Epoch 12/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7383 - categorical_accuracy: 0.7054\n",
      "Epoch 00012: saving model to model_init_2025-03-0407_08_44.828734/model-00012-0.73827-0.70536-0.64555-0.75781.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.7383 - categorical_accuracy: 0.7054 - val_loss: 0.6456 - val_categorical_accuracy: 0.7578 - lr: 0.0010\n",
      "Epoch 13/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7000 - categorical_accuracy: 0.7188\n",
      "Epoch 00013: saving model to model_init_2025-03-0407_08_44.828734/model-00013-0.70003-0.71875-0.68897-0.77344.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.7000 - categorical_accuracy: 0.7188 - val_loss: 0.6890 - val_categorical_accuracy: 0.7734 - lr: 0.0010\n",
      "Epoch 14/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4110 - categorical_accuracy: 0.8631\n",
      "Epoch 00017: saving model to model_init_2025-03-0407_08_44.828734/model-00017-0.41105-0.86310-0.52642-0.80469.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 0.4110 - categorical_accuracy: 0.8631 - val_loss: 0.5264 - val_categorical_accuracy: 0.8047 - lr: 2.0000e-04\n",
      "Epoch 18/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4318 - categorical_accuracy: 0.8199\n",
      "Epoch 00018: saving model to model_init_2025-03-0407_08_44.828734/model-00018-0.43182-0.81994-0.51486-0.78125.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.4318 - categorical_accuracy: 0.8199 - val_loss: 0.5149 - val_categorical_accuracy: 0.7812 - lr: 2.0000e-04\n",
      "Epoch 19/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3981 - categorical_accuracy: 0.8423\n",
      "Epoch 00019: saving model to model_init_2025-03-0407_08_44.828734/model-00019-0.39810-0.84226-0.57688-0.77344.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.3981 - categorical_accuracy: 0.8423 - val_loss: 0.5769 - val_categorical_accuracy: 0.7734 - lr: 2.0000e-04\n",
      "Epoch 20/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4058 - categorical_accuracy: 0.8378\n",
      "Epoch 00020: saving model to model_init_2025-03-0407_08_44.828734/model-00020-0.40577-0.83780-0.53754-0.77344.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 0.4058 - categorical_accuracy: 0.8378 - val_loss: 0.5375 - val_categorical_accuracy: 0.7734 - lr: 2.0000e-04\n",
      "Epoch 21/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3520 - categorical_accuracy: 0.8631\n",
      "Epoch 00021: saving model to model_init_2025-03-0407_08_44.828734/model-00021-0.35196-0.86310-0.54308-0.76562.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.3520 - categorical_accuracy: 0.8631 - val_loss: 0.5431 - val_categorical_accuracy: 0.7656 - lr: 2.0000e-04\n",
      "Epoch 22/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3355 - categorical_accuracy: 0.8765\n",
      "Epoch 00022: saving model to model_init_2025-03-0407_08_44.828734/model-00022-0.33555-0.87649-0.54917-0.79688.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.3355 - categorical_accuracy: 0.8765 - val_loss: 0.5492 - val_categorical_accuracy: 0.7969 - lr: 2.0000e-04\n",
      "Epoch 23/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3154 - categorical_accuracy: 0.8735\n",
      "Epoch 00023: saving model to model_init_2025-03-0407_08_44.828734/model-00023-0.31536-0.87351-0.45852-0.84375.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.3154 - categorical_accuracy: 0.8735 - val_loss: 0.4585 - val_categorical_accuracy: 0.8438 - lr: 4.0000e-05\n",
      "Epoch 24/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3293 - categorical_accuracy: 0.8750\n",
      "Epoch 00024: saving model to model_init_2025-03-0407_08_44.828734/model-00024-0.32932-0.87500-0.55130-0.79688.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 0.3293 - categorical_accuracy: 0.8750 - val_loss: 0.5513 - val_categorical_accuracy: 0.7969 - lr: 4.0000e-05\n",
      "Epoch 25/40\n",
      "10/21 [=============>................] - ETA: 32s - loss: 0.2809 - categorical_accuracy: 0.9000\n",
      "Epoch 00025: saving model to model_init_2025-03-0407_08_44.828734/model-00025-0.31548-0.88690-0.55909-0.78125.h5\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.3155 - categorical_accuracy: 0.8869 - val_loss: 0.5591 - val_categorical_accuracy: 0.7812 - lr: 4.0000e-05\n",
      "Epoch 26/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3241 - categorical_accuracy: 0.8780\n",
      "Epoch 00026: saving model to model_init_2025-03-0407_08_44.828734/model-00026-0.32412-0.87798-0.49928-0.81250.h5\n",
      "21/21 [==============================] - 78s 4s/step - loss: 0.3241 - categorical_accuracy: 0.8780 - val_loss: 0.4993 - val_categorical_accuracy: 0.8125 - lr: 4.0000e-05\n",
      "Epoch 27/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3069 - categorical_accuracy: 0.8839\n",
      "Epoch 00027: saving model to model_init_2025-03-0407_08_44.828734/model-00027-0.30693-0.88393-0.54868-0.75000.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.3069 - categorical_accuracy: 0.8839 - val_loss: 0.5487 - val_categorical_accuracy: 0.7500 - lr: 4.0000e-05\n",
      "Epoch 28/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2778 - categorical_accuracy: 0.9062\n",
      "Epoch 00028: saving model to model_init_2025-03-0407_08_44.828734/model-00028-0.27782-0.90625-0.56376-0.78125.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 0.2778 - categorical_accuracy: 0.9062 - val_loss: 0.5638 - val_categorical_accuracy: 0.7812 - lr: 8.0000e-06\n",
      "Epoch 29/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2991 - categorical_accuracy: 0.8869\n",
      "Epoch 00029: saving model to model_init_2025-03-0407_08_44.828734/model-00029-0.29909-0.88690-0.53944-0.78906.h5\n",
      "21/21 [==============================] - 77s 4s/step - loss: 0.2991 - categorical_accuracy: 0.8869 - val_loss: 0.5394 - val_categorical_accuracy: 0.7891 - lr: 8.0000e-06\n",
      "Epoch 30/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2980 - categorical_accuracy: 0.8810\n",
      "Epoch 00033: saving model to model_init_2025-03-0407_08_44.828734/model-00033-0.29801-0.88095-0.53150-0.78125.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.2980 - categorical_accuracy: 0.8810 - val_loss: 0.5315 - val_categorical_accuracy: 0.7812 - lr: 8.0000e-06\n",
      "Epoch 34/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2824 - categorical_accuracy: 0.9122\n",
      "Epoch 00034: saving model to model_init_2025-03-0407_08_44.828734/model-00034-0.28244-0.91220-0.63803-0.71875.h5\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "21/21 [==============================] - 77s 4s/step - loss: 0.2824 - categorical_accuracy: 0.9122 - val_loss: 0.6380 - val_categorical_accuracy: 0.7188 - lr: 8.0000e-06\n",
      "Epoch 35/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2760 - categorical_accuracy: 0.9033\n",
      "Epoch 00035: saving model to model_init_2025-03-0407_08_44.828734/model-00035-0.27605-0.90327-0.57650-0.75000.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.2760 - categorical_accuracy: 0.9033 - val_loss: 0.5765 - val_categorical_accuracy: 0.7500 - lr: 1.6000e-06\n",
      "Epoch 36/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3212 - categorical_accuracy: 0.8750\n",
      "Epoch 00036: saving model to model_init_2025-03-0407_08_44.828734/model-00036-0.32123-0.87500-0.56149-0.76562.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.3212 - categorical_accuracy: 0.8750 - val_loss: 0.5615 - val_categorical_accuracy: 0.7656 - lr: 1.6000e-06\n",
      "Epoch 37/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3176 - categorical_accuracy: 0.8735\n",
      "Epoch 00040: saving model to model_init_2025-03-0407_08_44.828734/model-00040-0.30130-0.88393-0.47912-0.79688.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.3013 - categorical_accuracy: 0.8839 - val_loss: 0.4791 - val_categorical_accuracy: 0.7969 - lr: 3.2000e-07\n"
     ]
    }
   ],
   "source": [
    "num_epochs3=40\n",
    "history4 = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs3, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "Tried with number of epochs =40 with batch size=32.\n",
    "Model Result: The model shows consistent improvement in both training and validation accuracy, but signs of overfitting emerge in later epochs so far this Con3D model gave best accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fifth Model-TimeDistributed + Conv2D + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 11:30:32.882827: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2025-03-04 11:30:32.882890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22855 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:1c:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, None, 78, 78, 32)  896      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, None, 39, 39, 32)  0        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, None, 37, 37, 64)  18496    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, None, 18, 18, 64)  0        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, None, 16, 16, 128  73856    \n",
      " tributed)                   )                                   \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, None, 8, 8, 128)  0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, None, 8192)       0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 128)         4260352   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,407,493\n",
      "Trainable params: 4,407,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Time Distributed Conv2D layers for spatial feature extraction\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(None, 80, 80, 3)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))  # Flatten spatial features for each frame\n",
    "\n",
    "# LSTM layers for temporal modeling\n",
    "model.add(LSTM(128, return_sequences=True))  # First LSTM layer\n",
    "model.add(LSTM(64))  # Second LSTM layer\n",
    "\n",
    "# Fully connected layers for classification\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Dropout for regularization\n",
    "model.add(Dense(5,activation='softmax'))  # Output layer\n",
    "\n",
    "# compile the model\n",
    "optimiser = 'adam' #write your optimizer''\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',save_freq = 'epoch')\n",
    "\n",
    "LR =ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 11:31:44.586279: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - ETA: 0s - loss: 1.6165 - categorical_accuracy: 0.2232Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_init_2025-03-0411_26_01.292544/model-00001-1.61651-0.22321-1.58128-0.25781.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 1.6165 - categorical_accuracy: 0.2232 - val_loss: 1.5813 - val_categorical_accuracy: 0.2578 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5088 - categorical_accuracy: 0.3289\n",
      "Epoch 00002: saving model to model_init_2025-03-0411_26_01.292544/model-00002-1.50881-0.32887-1.42635-0.42188.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 1.5088 - categorical_accuracy: 0.3289 - val_loss: 1.4264 - val_categorical_accuracy: 0.4219 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2936 - categorical_accuracy: 0.4583\n",
      "Epoch 00003: saving model to model_init_2025-03-0411_26_01.292544/model-00003-1.29357-0.45833-1.02721-0.59375.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 1.2936 - categorical_accuracy: 0.4583 - val_loss: 1.0272 - val_categorical_accuracy: 0.5938 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0698 - categorical_accuracy: 0.5699\n",
      "Epoch 00004: saving model to model_init_2025-03-0411_26_01.292544/model-00004-1.06975-0.56994-1.12861-0.52344.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 1.0698 - categorical_accuracy: 0.5699 - val_loss: 1.1286 - val_categorical_accuracy: 0.5234 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6002 - categorical_accuracy: 0.7932\n",
      "Epoch 00008: saving model to model_init_2025-03-0411_26_01.292544/model-00008-0.60018-0.79315-1.28811-0.59375.h5\n",
      "21/21 [==============================] - 70s 4s/step - loss: 0.6002 - categorical_accuracy: 0.7932 - val_loss: 1.2881 - val_categorical_accuracy: 0.5938 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5185 - categorical_accuracy: 0.8214\n",
      "Epoch 00009: saving model to model_init_2025-03-0411_26_01.292544/model-00009-0.51852-0.82143-1.09956-0.67969.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.5185 - categorical_accuracy: 0.8214 - val_loss: 1.0996 - val_categorical_accuracy: 0.6797 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2984 - categorical_accuracy: 0.9048\n",
      "Epoch 00010: saving model to model_init_2025-03-0411_26_01.292544/model-00010-0.29842-0.90476-1.23869-0.68750.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 0.2984 - categorical_accuracy: 0.9048 - val_loss: 1.2387 - val_categorical_accuracy: 0.6875 - lr: 2.0000e-04\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1917 - categorical_accuracy: 0.9435\n",
      "Epoch 00012: saving model to model_init_2025-03-0411_26_01.292544/model-00012-0.19175-0.94345-1.20591-0.69531.h5\n",
      "21/21 [==============================] - 70s 4s/step - loss: 0.1917 - categorical_accuracy: 0.9435 - val_loss: 1.2059 - val_categorical_accuracy: 0.6953 - lr: 2.0000e-04\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1761 - categorical_accuracy: 0.9554\n",
      "Epoch 00013: saving model to model_init_2025-03-0411_26_01.292544/model-00013-0.17608-0.95536-1.16668-0.71875.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "21/21 [==============================] - 69s 3s/step - loss: 0.1761 - categorical_accuracy: 0.9554 - val_loss: 1.1667 - val_categorical_accuracy: 0.7188 - lr: 2.0000e-04\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1346 - categorical_accuracy: 0.9554\n",
      "Epoch 00014: saving model to model_init_2025-03-0411_26_01.292544/model-00014-0.13464-0.95536-1.21592-0.69531.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.1346 - categorical_accuracy: 0.9554 - val_loss: 1.2159 - val_categorical_accuracy: 0.6953 - lr: 4.0000e-05\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1175 - categorical_accuracy: 0.9673\n",
      "Epoch 00015: saving model to model_init_2025-03-0411_26_01.292544/model-00015-0.11754-0.96726-1.19689-0.71094.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.1175 - categorical_accuracy: 0.9673 - val_loss: 1.1969 - val_categorical_accuracy: 0.7109 - lr: 4.0000e-05\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1106 - categorical_accuracy: 0.9702\n",
      "Epoch 00016: saving model to model_init_2025-03-0411_26_01.292544/model-00016-0.11058-0.97024-1.17887-0.71875.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.1106 - categorical_accuracy: 0.9702 - val_loss: 1.1789 - val_categorical_accuracy: 0.7188 - lr: 4.0000e-05\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1126 - categorical_accuracy: 0.9673\n",
      "Epoch 00017: saving model to model_init_2025-03-0411_26_01.292544/model-00017-0.11255-0.96726-1.14290-0.74219.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "21/21 [==============================] - 70s 4s/step - loss: 0.1126 - categorical_accuracy: 0.9673 - val_loss: 1.1429 - val_categorical_accuracy: 0.7422 - lr: 4.0000e-05\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1024 - categorical_accuracy: 0.9732\n",
      "Epoch 00018: saving model to model_init_2025-03-0411_26_01.292544/model-00018-0.10240-0.97321-1.20051-0.71094.h5\n",
      "21/21 [==============================] - 70s 3s/step - loss: 0.1024 - categorical_accuracy: 0.9732 - val_loss: 1.2005 - val_categorical_accuracy: 0.7109 - lr: 8.0000e-06\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1037 - categorical_accuracy: 0.9762\n",
      "Epoch 00019: saving model to model_init_2025-03-0411_26_01.292544/model-00019-0.10370-0.97619-1.28876-0.70312.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.1037 - categorical_accuracy: 0.9762 - val_loss: 1.2888 - val_categorical_accuracy: 0.7031 - lr: 8.0000e-06\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0937 - categorical_accuracy: 0.9821\n",
      "Epoch 00020: saving model to model_init_2025-03-0411_26_01.292544/model-00020-0.09367-0.98214-1.17751-0.71875.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 0.0937 - categorical_accuracy: 0.9821 - val_loss: 1.1775 - val_categorical_accuracy: 0.7188 - lr: 8.0000e-06\n"
     ]
    }
   ],
   "source": [
    "history5=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "Overfitting: The large gap between training and validation accuracy (e.g., 98.21% vs. 71.88%) suggests that the model is overfitting to the training data.\n",
    "\n",
    "Validation Performance: The validation accuracy plateaus around 70-72%, indicating that the model struggles to generalize to unseen data.\n",
    "\n",
    "Learning Rate: The learning rate adjustments helped stabilize training but did not significantly improve validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sixth Model-Conv2D + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 14:28:37.521030: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2025-03-04 14:28:37.521118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14800 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:41:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, None, 78, 78, 32)  896      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, None, 39, 39, 32)  0        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, None, 37, 37, 64)  18496    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, None, 37, 37, 64)  256      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, None, 18, 18, 64)  0        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, None, 20736)      0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 64)          3993984   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                9408      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,025,477\n",
      "Trainable params: 4,025,349\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Time Distributed Conv2D layers for spatial feature extraction\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(None, 80, 80, 3)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))  # Flatten spatial features for each frame\n",
    "\n",
    "# GRU layers for temporal modeling\n",
    "model.add(GRU(64,return_sequences=True))  # First GRU layer\n",
    "model.add(GRU(32, return_sequences=False))  # Second GRU layer\n",
    "\n",
    "# Fully connected layers for classification\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(5, activation='softmax'))  # Output layer (5 classes)\n",
    "\n",
    "# Compile the model\n",
    "optimiser = 'adam'  # Optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',save_freq = 'epoch')\n",
    "\n",
    "LR =ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 14:29:08.807106: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - ETA: 0s - loss: 1.5618 - categorical_accuracy: 0.3125Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_init_2025-03-0414_25_22.012483/model-00001-1.56177-0.31250-1.61561-0.21875.h5\n",
      "21/21 [==============================] - 79s 4s/step - loss: 1.5618 - categorical_accuracy: 0.3125 - val_loss: 1.6156 - val_categorical_accuracy: 0.2188 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2874 - categorical_accuracy: 0.4866\n",
      "Epoch 00002: saving model to model_init_2025-03-0414_25_22.012483/model-00002-1.28745-0.48661-1.60130-0.21875.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 1.2874 - categorical_accuracy: 0.4866 - val_loss: 1.6013 - val_categorical_accuracy: 0.2188 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0767 - categorical_accuracy: 0.5848\n",
      "Epoch 00003: saving model to model_init_2025-03-0414_25_22.012483/model-00003-1.07671-0.58482-1.45903-0.26562.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 1.0767 - categorical_accuracy: 0.5848 - val_loss: 1.4590 - val_categorical_accuracy: 0.2656 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8758 - categorical_accuracy: 0.7068\n",
      "Epoch 00004: saving model to model_init_2025-03-0414_25_22.012483/model-00004-0.87580-0.70685-1.21535-0.57031.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.8758 - categorical_accuracy: 0.7068 - val_loss: 1.2153 - val_categorical_accuracy: 0.5703 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7011 - categorical_accuracy: 0.7768\n",
      "Epoch 00005: saving model to model_init_2025-03-0414_25_22.012483/model-00005-0.70106-0.77679-1.53130-0.41406.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.7011 - categorical_accuracy: 0.7768 - val_loss: 1.5313 - val_categorical_accuracy: 0.4141 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6065 - categorical_accuracy: 0.7917\n",
      "Epoch 00006: saving model to model_init_2025-03-0414_25_22.012483/model-00006-0.60646-0.79167-1.76749-0.34375.h5\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.6065 - categorical_accuracy: 0.7917 - val_loss: 1.7675 - val_categorical_accuracy: 0.3438 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4538 - categorical_accuracy: 0.8557\n",
      "Epoch 00007: saving model to model_init_2025-03-0414_25_22.012483/model-00007-0.45381-0.85565-1.17083-0.53125.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.4538 - categorical_accuracy: 0.8557 - val_loss: 1.1708 - val_categorical_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3616 - categorical_accuracy: 0.9033\n",
      "Epoch 00008: saving model to model_init_2025-03-0414_25_22.012483/model-00008-0.36163-0.90327-2.03224-0.41406.h5\n",
      "21/21 [==============================] - 68s 3s/step - loss: 0.3616 - categorical_accuracy: 0.9033 - val_loss: 2.0322 - val_categorical_accuracy: 0.4141 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2447 - categorical_accuracy: 0.9226\n",
      "Epoch 00009: saving model to model_init_2025-03-0414_25_22.012483/model-00009-0.24469-0.92262-2.12411-0.45312.h5\n",
      "21/21 [==============================] - 69s 3s/step - loss: 0.2447 - categorical_accuracy: 0.9226 - val_loss: 2.1241 - val_categorical_accuracy: 0.4531 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2078 - categorical_accuracy: 0.9375\n",
      "Epoch 00010: saving model to model_init_2025-03-0414_25_22.012483/model-00010-0.20783-0.93750-1.11075-0.63281.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.2078 - categorical_accuracy: 0.9375 - val_loss: 1.1108 - val_categorical_accuracy: 0.6328 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1273 - categorical_accuracy: 0.9777\n",
      "Epoch 00011: saving model to model_init_2025-03-0414_25_22.012483/model-00011-0.12731-0.97768-1.71727-0.56250.h5\n",
      "21/21 [==============================] - 70s 3s/step - loss: 0.1273 - categorical_accuracy: 0.9777 - val_loss: 1.7173 - val_categorical_accuracy: 0.5625 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0870 - categorical_accuracy: 0.9792\n",
      "Epoch 00012: saving model to model_init_2025-03-0414_25_22.012483/model-00012-0.08701-0.97917-1.70671-0.57031.h5\n",
      "21/21 [==============================] - 69s 3s/step - loss: 0.0870 - categorical_accuracy: 0.9792 - val_loss: 1.7067 - val_categorical_accuracy: 0.5703 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0996 - categorical_accuracy: 0.9777\n",
      "Epoch 00013: saving model to model_init_2025-03-0414_25_22.012483/model-00013-0.09960-0.97768-2.04973-0.47656.h5\n",
      "21/21 [==============================] - 68s 3s/step - loss: 0.0996 - categorical_accuracy: 0.9777 - val_loss: 2.0497 - val_categorical_accuracy: 0.4766 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0879 - categorical_accuracy: 0.9792\n",
      "Epoch 00014: saving model to model_init_2025-03-0414_25_22.012483/model-00014-0.08792-0.97917-1.91644-0.55469.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "21/21 [==============================] - 70s 4s/step - loss: 0.0879 - categorical_accuracy: 0.9792 - val_loss: 1.9164 - val_categorical_accuracy: 0.5547 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0897 - categorical_accuracy: 0.9747\n",
      "Epoch 00015: saving model to model_init_2025-03-0414_25_22.012483/model-00015-0.08966-0.97470-2.21467-0.53906.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 0.0897 - categorical_accuracy: 0.9747 - val_loss: 2.2147 - val_categorical_accuracy: 0.5391 - lr: 2.0000e-04\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0530 - categorical_accuracy: 0.9926\n",
      "Epoch 00016: saving model to model_init_2025-03-0414_25_22.012483/model-00016-0.05305-0.99256-1.74718-0.60938.h5\n",
      "21/21 [==============================] - 70s 3s/step - loss: 0.0530 - categorical_accuracy: 0.9926 - val_loss: 1.7472 - val_categorical_accuracy: 0.6094 - lr: 2.0000e-04\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0367 - categorical_accuracy: 0.9970\n",
      "Epoch 00017: saving model to model_init_2025-03-0414_25_22.012483/model-00017-0.03671-0.99702-1.46801-0.67188.h5\n",
      "21/21 [==============================] - 70s 3s/step - loss: 0.0367 - categorical_accuracy: 0.9970 - val_loss: 1.4680 - val_categorical_accuracy: 0.6719 - lr: 2.0000e-04\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0333 - categorical_accuracy: 0.9985\n",
      "Epoch 00018: saving model to model_init_2025-03-0414_25_22.012483/model-00018-0.03332-0.99851-1.45194-0.65625.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "21/21 [==============================] - 79s 4s/step - loss: 0.0333 - categorical_accuracy: 0.9985 - val_loss: 1.4519 - val_categorical_accuracy: 0.6562 - lr: 2.0000e-04\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0309 - categorical_accuracy: 0.9985\n",
      "Epoch 00019: saving model to model_init_2025-03-0414_25_22.012483/model-00019-0.03092-0.99851-1.44025-0.71094.h5\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.0309 - categorical_accuracy: 0.9985 - val_loss: 1.4403 - val_categorical_accuracy: 0.7109 - lr: 4.0000e-05\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0326 - categorical_accuracy: 1.0000\n",
      "Epoch 00020: saving model to model_init_2025-03-0414_25_22.012483/model-00020-0.03259-1.00000-1.50578-0.64844.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.0326 - categorical_accuracy: 1.0000 - val_loss: 1.5058 - val_categorical_accuracy: 0.6484 - lr: 4.0000e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history6=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "Model result: Overfitting\n",
    "The training accuracy reached 100%, while the validation accuracy plateaued around 60-65%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seventh Model-Conv2D + LSTM with Minimal filters at each Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_19 (TimeDi  (None, None, 78, 78, 8)  224       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_20 (TimeDi  (None, None, 39, 39, 8)  0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_21 (TimeDi  (None, None, 37, 37, 16)  1168     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_22 (TimeDi  (None, None, 18, 18, 16)  0        \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_23 (TimeDi  (None, None, 16, 16, 32)  4640     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_24 (TimeDi  (None, None, 8, 8, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_25 (TimeDi  (None, None, 2048)       0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 16)          132160    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                6272      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145,077\n",
      "Trainable params: 145,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Time Distributed Conv2D layers for spatial feature extraction\n",
    "model.add(TimeDistributed(Conv2D(8, (3, 3), activation='relu'), input_shape=(None, 80, 80, 3)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))  # Flatten spatial features for each frame\n",
    "\n",
    "# LSTM layers for temporal modeling\n",
    "model.add(LSTM(16, return_sequences=True))  # First LSTM layer\n",
    "model.add(LSTM(32))  # Second LSTM layer\n",
    "\n",
    "\n",
    "\n",
    "# Fully connected layers for classification\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.25))  # Dropout for regularization\n",
    "model.add(Dense(5,activation='softmax'))  # Output layer\n",
    "\n",
    "# compile the model\n",
    "optimiser = 'adam' #write your optimizer''\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',save_freq = 'epoch')\n",
    "\n",
    "LR =ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 16:34:00.191603: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - ETA: 0s - loss: 1.6107 - categorical_accuracy: 0.1935Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_init_2025-03-0416_28_47.540319/model-00001-1.61071-0.19345-1.60215-0.22656.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 1.6107 - categorical_accuracy: 0.1935 - val_loss: 1.6021 - val_categorical_accuracy: 0.2266 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5962 - categorical_accuracy: 0.2188\n",
      "Epoch 00002: saving model to model_init_2025-03-0416_28_47.540319/model-00002-1.59620-0.21875-1.58270-0.28906.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 1.5962 - categorical_accuracy: 0.2188 - val_loss: 1.5827 - val_categorical_accuracy: 0.2891 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5063 - categorical_accuracy: 0.3036\n",
      "Epoch 00003: saving model to model_init_2025-03-0416_28_47.540319/model-00003-1.50633-0.30357-1.48594-0.35156.h5\n",
      "21/21 [==============================] - 69s 3s/step - loss: 1.5063 - categorical_accuracy: 0.3036 - val_loss: 1.4859 - val_categorical_accuracy: 0.3516 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4401 - categorical_accuracy: 0.3512\n",
      "Epoch 00004: saving model to model_init_2025-03-0416_28_47.540319/model-00004-1.44009-0.35119-1.39532-0.38281.h5\n",
      "21/21 [==============================] - 69s 3s/step - loss: 1.4401 - categorical_accuracy: 0.3512 - val_loss: 1.3953 - val_categorical_accuracy: 0.3828 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2809 - categorical_accuracy: 0.4539\n",
      "Epoch 00005: saving model to model_init_2025-03-0416_28_47.540319/model-00005-1.28091-0.45387-1.21776-0.42969.h5\n",
      "21/21 [==============================] - 75s 4s/step - loss: 1.2809 - categorical_accuracy: 0.4539 - val_loss: 1.2178 - val_categorical_accuracy: 0.4297 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1887 - categorical_accuracy: 0.4970\n",
      "Epoch 00006: saving model to model_init_2025-03-0416_28_47.540319/model-00006-1.18870-0.49702-1.21430-0.48438.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 1.1887 - categorical_accuracy: 0.4970 - val_loss: 1.2143 - val_categorical_accuracy: 0.4844 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1287 - categorical_accuracy: 0.5387\n",
      "Epoch 00007: saving model to model_init_2025-03-0416_28_47.540319/model-00007-1.12869-0.53869-1.18614-0.55469.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 1.1287 - categorical_accuracy: 0.5387 - val_loss: 1.1861 - val_categorical_accuracy: 0.5547 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9744 - categorical_accuracy: 0.6057\n",
      "Epoch 00008: saving model to model_init_2025-03-0416_28_47.540319/model-00008-0.97441-0.60565-1.15266-0.52344.h5\n",
      "21/21 [==============================] - 69s 3s/step - loss: 0.9744 - categorical_accuracy: 0.6057 - val_loss: 1.1527 - val_categorical_accuracy: 0.5234 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9343 - categorical_accuracy: 0.6399\n",
      "Epoch 00009: saving model to model_init_2025-03-0416_28_47.540319/model-00009-0.93429-0.63988-1.10419-0.53125.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.9343 - categorical_accuracy: 0.6399 - val_loss: 1.1042 - val_categorical_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8408 - categorical_accuracy: 0.6860\n",
      "Epoch 00010: saving model to model_init_2025-03-0416_28_47.540319/model-00010-0.84080-0.68601-1.08611-0.53906.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 0.8408 - categorical_accuracy: 0.6860 - val_loss: 1.0861 - val_categorical_accuracy: 0.5391 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7441 - categorical_accuracy: 0.7292\n",
      "Epoch 00011: saving model to model_init_2025-03-0416_28_47.540319/model-00011-0.74409-0.72917-1.32298-0.46875.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.7441 - categorical_accuracy: 0.7292 - val_loss: 1.3230 - val_categorical_accuracy: 0.4688 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7145 - categorical_accuracy: 0.7515\n",
      "Epoch 00012: saving model to model_init_2025-03-0416_28_47.540319/model-00012-0.71451-0.75149-1.20742-0.56250.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.7145 - categorical_accuracy: 0.7515 - val_loss: 1.2074 - val_categorical_accuracy: 0.5625 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7529 - categorical_accuracy: 0.7039\n",
      "Epoch 00013: saving model to model_init_2025-03-0416_28_47.540319/model-00013-0.75289-0.70387-1.50194-0.48438.h5\n",
      "21/21 [==============================] - 70s 4s/step - loss: 0.7529 - categorical_accuracy: 0.7039 - val_loss: 1.5019 - val_categorical_accuracy: 0.4844 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6603 - categorical_accuracy: 0.7649\n",
      "Epoch 00014: saving model to model_init_2025-03-0416_28_47.540319/model-00014-0.66029-0.76488-1.38522-0.55469.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.6603 - categorical_accuracy: 0.7649 - val_loss: 1.3852 - val_categorical_accuracy: 0.5547 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5335 - categorical_accuracy: 0.8318\n",
      "Epoch 00015: saving model to model_init_2025-03-0416_28_47.540319/model-00015-0.53347-0.83185-1.17714-0.60938.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.5335 - categorical_accuracy: 0.8318 - val_loss: 1.1771 - val_categorical_accuracy: 0.6094 - lr: 2.0000e-04\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4602 - categorical_accuracy: 0.8616\n",
      "Epoch 00016: saving model to model_init_2025-03-0416_28_47.540319/model-00016-0.46024-0.86161-1.12572-0.60938.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.4602 - categorical_accuracy: 0.8616 - val_loss: 1.1257 - val_categorical_accuracy: 0.6094 - lr: 2.0000e-04\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4424 - categorical_accuracy: 0.8646\n",
      "Epoch 00017: saving model to model_init_2025-03-0416_28_47.540319/model-00017-0.44243-0.86458-1.08844-0.66406.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.4424 - categorical_accuracy: 0.8646 - val_loss: 1.0884 - val_categorical_accuracy: 0.6641 - lr: 2.0000e-04\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4169 - categorical_accuracy: 0.8914\n",
      "Epoch 00018: saving model to model_init_2025-03-0416_28_47.540319/model-00018-0.41690-0.89137-1.20135-0.59375.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.4169 - categorical_accuracy: 0.8914 - val_loss: 1.2014 - val_categorical_accuracy: 0.5938 - lr: 2.0000e-04\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3677 - categorical_accuracy: 0.8958\n",
      "Epoch 00019: saving model to model_init_2025-03-0416_28_47.540319/model-00019-0.36768-0.89583-1.23709-0.58594.h5\n",
      "21/21 [==============================] - 75s 4s/step - loss: 0.3677 - categorical_accuracy: 0.8958 - val_loss: 1.2371 - val_categorical_accuracy: 0.5859 - lr: 4.0000e-05\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3755 - categorical_accuracy: 0.8884\n",
      "Epoch 00020: saving model to model_init_2025-03-0416_28_47.540319/model-00020-0.37546-0.88839-1.16519-0.60156.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 0.3755 - categorical_accuracy: 0.8884 - val_loss: 1.1652 - val_categorical_accuracy: 0.6016 - lr: 4.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history7=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "Tried with the less filters at each Conv2D layers but still the model result is showing Overfitting\n",
    "Lets try the same with Model with GRU to see if any increase in Validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigth Model - Conv2D + GRU with minimal filters at Conv2D Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_26 (TimeDi  (None, None, 78, 78, 8)  224       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_27 (TimeDi  (None, None, 39, 39, 8)  0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_28 (TimeDi  (None, None, 37, 37, 16)  1168     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_29 (TimeDi  (None, None, 18, 18, 16)  0        \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_30 (TimeDi  (None, None, 16, 16, 32)  4640     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_31 (TimeDi  (None, None, 8, 8, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_32 (TimeDi  (None, None, 2048)       0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 64)          405888    \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                9408      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 422,549\n",
      "Trainable params: 422,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Time Distributed Conv2D layers for spatial feature extraction\n",
    "model.add(TimeDistributed(Conv2D(8, (3, 3), activation='relu'), input_shape=(None, 80, 80, 3)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))  # Flatten spatial features for each frame\n",
    "\n",
    "# GRU layers for temporal modeling\n",
    "model.add(GRU(64,return_sequences=True))  # First GRU layer\n",
    "model.add(GRU(32, return_sequences=False))  # Second GRU layer\n",
    "\n",
    "\n",
    "# Fully connected layers for classification\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(5,activation='softmax'))  # Output layer\n",
    "\n",
    "# compile the model\n",
    "optimiser = 'adam' #write your optimizer''\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',save_freq = 'epoch')\n",
    "\n",
    "LR =ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6038 - categorical_accuracy: 0.2292Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_init_2025-03-0416_28_47.540319/model-00001-1.60385-0.22917-1.57178-0.25000.h5\n",
      "21/21 [==============================] - 76s 4s/step - loss: 1.6038 - categorical_accuracy: 0.2292 - val_loss: 1.5718 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5506 - categorical_accuracy: 0.2768\n",
      "Epoch 00002: saving model to model_init_2025-03-0416_28_47.540319/model-00002-1.55060-0.27679-1.49128-0.36719.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 1.5506 - categorical_accuracy: 0.2768 - val_loss: 1.4913 - val_categorical_accuracy: 0.3672 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4246 - categorical_accuracy: 0.3929\n",
      "Epoch 00003: saving model to model_init_2025-03-0416_28_47.540319/model-00003-1.42461-0.39286-1.32827-0.39844.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 1.4246 - categorical_accuracy: 0.3929 - val_loss: 1.3283 - val_categorical_accuracy: 0.3984 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2119 - categorical_accuracy: 0.5283\n",
      "Epoch 00004: saving model to model_init_2025-03-0416_28_47.540319/model-00004-1.21194-0.52827-1.16510-0.57812.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 1.2119 - categorical_accuracy: 0.5283 - val_loss: 1.1651 - val_categorical_accuracy: 0.5781 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0400 - categorical_accuracy: 0.6071\n",
      "Epoch 00005: saving model to model_init_2025-03-0416_28_47.540319/model-00005-1.04001-0.60714-1.09178-0.56250.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 1.0400 - categorical_accuracy: 0.6071 - val_loss: 1.0918 - val_categorical_accuracy: 0.5625 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9014 - categorical_accuracy: 0.6577\n",
      "Epoch 00006: saving model to model_init_2025-03-0416_28_47.540319/model-00006-0.90139-0.65774-1.03087-0.59375.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.9014 - categorical_accuracy: 0.6577 - val_loss: 1.0309 - val_categorical_accuracy: 0.5938 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7828 - categorical_accuracy: 0.7083\n",
      "Epoch 00007: saving model to model_init_2025-03-0416_28_47.540319/model-00007-0.78277-0.70833-1.05669-0.61719.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.7828 - categorical_accuracy: 0.7083 - val_loss: 1.0567 - val_categorical_accuracy: 0.6172 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6973 - categorical_accuracy: 0.7515\n",
      "Epoch 00008: saving model to model_init_2025-03-0416_28_47.540319/model-00008-0.69725-0.75149-1.00167-0.62500.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 0.6973 - categorical_accuracy: 0.7515 - val_loss: 1.0017 - val_categorical_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5741 - categorical_accuracy: 0.8140\n",
      "Epoch 00009: saving model to model_init_2025-03-0416_28_47.540319/model-00009-0.57413-0.81399-1.24062-0.54688.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 0.5741 - categorical_accuracy: 0.8140 - val_loss: 1.2406 - val_categorical_accuracy: 0.5469 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5184 - categorical_accuracy: 0.8259\n",
      "Epoch 00010: saving model to model_init_2025-03-0416_28_47.540319/model-00010-0.51840-0.82589-1.00184-0.66406.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.5184 - categorical_accuracy: 0.8259 - val_loss: 1.0018 - val_categorical_accuracy: 0.6641 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4660 - categorical_accuracy: 0.8586\n",
      "Epoch 00011: saving model to model_init_2025-03-0416_28_47.540319/model-00011-0.46602-0.85863-0.97081-0.75781.h5\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.4660 - categorical_accuracy: 0.8586 - val_loss: 0.9708 - val_categorical_accuracy: 0.7578 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4824 - categorical_accuracy: 0.8229\n",
      "Epoch 00012: saving model to model_init_2025-03-0416_28_47.540319/model-00012-0.48237-0.82292-1.19248-0.67969.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.4824 - categorical_accuracy: 0.8229 - val_loss: 1.1925 - val_categorical_accuracy: 0.6797 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3672 - categorical_accuracy: 0.8854\n",
      "Epoch 00013: saving model to model_init_2025-03-0416_28_47.540319/model-00013-0.36720-0.88542-0.94668-0.69531.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.3672 - categorical_accuracy: 0.8854 - val_loss: 0.9467 - val_categorical_accuracy: 0.6953 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2807 - categorical_accuracy: 0.9152\n",
      "Epoch 00014: saving model to model_init_2025-03-0416_28_47.540319/model-00014-0.28067-0.91518-0.96834-0.68750.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.2807 - categorical_accuracy: 0.9152 - val_loss: 0.9683 - val_categorical_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2584 - categorical_accuracy: 0.9122\n",
      "Epoch 00015: saving model to model_init_2025-03-0416_28_47.540319/model-00015-0.25841-0.91220-1.48811-0.61719.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 0.2584 - categorical_accuracy: 0.9122 - val_loss: 1.4881 - val_categorical_accuracy: 0.6172 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2788 - categorical_accuracy: 0.9107\n",
      "Epoch 00016: saving model to model_init_2025-03-0416_28_47.540319/model-00016-0.27881-0.91071-1.27388-0.64062.h5\n",
      "21/21 [==============================] - 70s 4s/step - loss: 0.2788 - categorical_accuracy: 0.9107 - val_loss: 1.2739 - val_categorical_accuracy: 0.6406 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2001 - categorical_accuracy: 0.9464\n",
      "Epoch 00017: saving model to model_init_2025-03-0416_28_47.540319/model-00017-0.20010-0.94643-1.42170-0.62500.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "21/21 [==============================] - 73s 4s/step - loss: 0.2001 - categorical_accuracy: 0.9464 - val_loss: 1.4217 - val_categorical_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1523 - categorical_accuracy: 0.9628\n",
      "Epoch 00018: saving model to model_init_2025-03-0416_28_47.540319/model-00018-0.15225-0.96280-1.21153-0.68750.h5\n",
      "21/21 [==============================] - 74s 4s/step - loss: 0.1523 - categorical_accuracy: 0.9628 - val_loss: 1.2115 - val_categorical_accuracy: 0.6875 - lr: 2.0000e-04\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1169 - categorical_accuracy: 0.9836\n",
      "Epoch 00019: saving model to model_init_2025-03-0416_28_47.540319/model-00019-0.11690-0.98363-1.08858-0.72656.h5\n",
      "21/21 [==============================] - 72s 4s/step - loss: 0.1169 - categorical_accuracy: 0.9836 - val_loss: 1.0886 - val_categorical_accuracy: 0.7266 - lr: 2.0000e-04\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0897 - categorical_accuracy: 0.9896\n",
      "Epoch 00020: saving model to model_init_2025-03-0416_28_47.540319/model-00020-0.08968-0.98958-1.11284-0.73438.h5\n",
      "21/21 [==============================] - 71s 4s/step - loss: 0.0897 - categorical_accuracy: 0.9896 - val_loss: 1.1128 - val_categorical_accuracy: 0.7344 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history8=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ninth Model - Conv3D Model with more images:\n",
    "Previously we tried with the 80 * 80 image resolution and only 13 images from the set we considered. Now we will use same image resolution and we will use 20 images from each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [0,2,4,6,8,10,11,12,13,14,15,16,17,18,19,20,22,24,26,28]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t) // batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,20,80,80,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    image = resize(image,(80,80)).astype(np.float32)\n",
    "                    \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if len(t) % batch_size != 0:\n",
    "            batch_data = np.zeros((batch_size,20,80,80,3)) \n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = resize(image,(80,80)).astype(np.float32)\n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1 \n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield does    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from skimage.io import imread\n",
    "\n",
    "#write your model here\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (3, 3, 3), padding='same',\n",
    "         input_shape=(20,80,80,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = new_generator(train_path, train_doc, batch_size)\n",
    "val_generator = new_generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_3 (Conv3D)           (None, 20, 80, 80, 32)    2624      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 20, 80, 80, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 20, 80, 80, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 10, 40, 40, 32)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 10, 40, 40, 64)    16448     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 10, 40, 40, 64)    0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 10, 40, 40, 64)   256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 5, 20, 20, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 5, 20, 20, 128)    65664     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 5, 20, 20, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 5, 20, 20, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 2, 10, 10, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25600)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               3276928   \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,371,909\n",
      "Trainable params: 3,371,077\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "optimiser = 'sgd'\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=0\n",
    "validation_steps=0\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6634 - categorical_accuracy: 0.4092Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_init_2025-03-0506_39_47.413366/model-00001-1.66340-0.40923-1.99692-0.18750.h5\n",
      "21/21 [==============================] - 115s 6s/step - loss: 1.6634 - categorical_accuracy: 0.4092 - val_loss: 1.9969 - val_categorical_accuracy: 0.1875 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2855 - categorical_accuracy: 0.5000\n",
      "Epoch 00002: saving model to model_init_2025-03-0506_39_47.413366/model-00002-1.28552-0.50000-2.29828-0.33594.h5\n",
      "21/21 [==============================] - 111s 6s/step - loss: 1.2855 - categorical_accuracy: 0.5000 - val_loss: 2.2983 - val_categorical_accuracy: 0.3359 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8606 - categorical_accuracy: 0.6741\n",
      "Epoch 00004: saving model to model_init_2025-03-0506_39_47.413366/model-00004-0.86064-0.67411-3.05868-0.21094.h5\n",
      "21/21 [==============================] - 112s 6s/step - loss: 0.8606 - categorical_accuracy: 0.6741 - val_loss: 3.0587 - val_categorical_accuracy: 0.2109 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8474 - categorical_accuracy: 0.6771\n",
      "Epoch 00005: saving model to model_init_2025-03-0506_39_47.413366/model-00005-0.84744-0.67708-3.41666-0.23438.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "21/21 [==============================] - 110s 6s/step - loss: 0.8474 - categorical_accuracy: 0.6771 - val_loss: 3.4167 - val_categorical_accuracy: 0.2344 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6389 - categorical_accuracy: 0.7604\n",
      "Epoch 00006: saving model to model_init_2025-03-0506_39_47.413366/model-00006-0.63889-0.76042-3.92373-0.21094.h5\n",
      "21/21 [==============================] - 115s 6s/step - loss: 0.6389 - categorical_accuracy: 0.7604 - val_loss: 3.9237 - val_categorical_accuracy: 0.2109 - lr: 0.0020\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5981 - categorical_accuracy: 0.7902\n",
      "Epoch 00007: saving model to model_init_2025-03-0506_39_47.413366/model-00007-0.59813-0.79018-4.66868-0.22656.h5\n",
      "21/21 [==============================] - 116s 6s/step - loss: 0.5981 - categorical_accuracy: 0.7902 - val_loss: 4.6687 - val_categorical_accuracy: 0.2266 - lr: 0.0020\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6060 - categorical_accuracy: 0.7887\n",
      "Epoch 00008: saving model to model_init_2025-03-0506_39_47.413366/model-00008-0.60603-0.78869-5.06224-0.23438.h5\n",
      "21/21 [==============================] - 113s 6s/step - loss: 0.6060 - categorical_accuracy: 0.7887 - val_loss: 5.0622 - val_categorical_accuracy: 0.2344 - lr: 0.0020\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5699 - categorical_accuracy: 0.7887\n",
      "Epoch 00009: saving model to model_init_2025-03-0506_39_47.413366/model-00009-0.56991-0.78869-5.46790-0.21875.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "21/21 [==============================] - 112s 6s/step - loss: 0.5699 - categorical_accuracy: 0.7887 - val_loss: 5.4679 - val_categorical_accuracy: 0.2188 - lr: 0.0020\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5406 - categorical_accuracy: 0.7961\n",
      "Epoch 00010: saving model to model_init_2025-03-0506_39_47.413366/model-00010-0.54060-0.79613-5.34612-0.27344.h5\n",
      "21/21 [==============================] - 112s 6s/step - loss: 0.5406 - categorical_accuracy: 0.7961 - val_loss: 5.3461 - val_categorical_accuracy: 0.2734 - lr: 4.0000e-04\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5487 - categorical_accuracy: 0.7872\n",
      "Epoch 00011: saving model to model_init_2025-03-0506_39_47.413366/model-00011-0.54868-0.78720-5.94569-0.18750.h5\n",
      "21/21 [==============================] - 110s 5s/step - loss: 0.5487 - categorical_accuracy: 0.7872 - val_loss: 5.9457 - val_categorical_accuracy: 0.1875 - lr: 4.0000e-04\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4981 - categorical_accuracy: 0.8170\n",
      "Epoch 00012: saving model to model_init_2025-03-0506_39_47.413366/model-00012-0.49805-0.81696-4.85918-0.23438.h5\n",
      "21/21 [==============================] - 109s 5s/step - loss: 0.4981 - categorical_accuracy: 0.8170 - val_loss: 4.8592 - val_categorical_accuracy: 0.2344 - lr: 4.0000e-04\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4948 - categorical_accuracy: 0.8199\n",
      "Epoch 00013: saving model to model_init_2025-03-0506_39_47.413366/model-00013-0.49478-0.81994-4.89717-0.22656.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "21/21 [==============================] - 106s 5s/step - loss: 0.4948 - categorical_accuracy: 0.8199 - val_loss: 4.8972 - val_categorical_accuracy: 0.2266 - lr: 4.0000e-04\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5487 - categorical_accuracy: 0.7857\n",
      "Epoch 00014: saving model to model_init_2025-03-0506_39_47.413366/model-00014-0.54865-0.78571-4.57402-0.22656.h5\n",
      "21/21 [==============================] - 110s 5s/step - loss: 0.5487 - categorical_accuracy: 0.7857 - val_loss: 4.5740 - val_categorical_accuracy: 0.2266 - lr: 8.0000e-05\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4939 - categorical_accuracy: 0.8333\n",
      "Epoch 00015: saving model to model_init_2025-03-0506_39_47.413366/model-00015-0.49389-0.83333-4.53714-0.25781.h5\n",
      "21/21 [==============================] - 112s 6s/step - loss: 0.4939 - categorical_accuracy: 0.8333 - val_loss: 4.5371 - val_categorical_accuracy: 0.2578 - lr: 8.0000e-05\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4939 - categorical_accuracy: 0.8125\n",
      "Epoch 00016: saving model to model_init_2025-03-0506_39_47.413366/model-00016-0.49393-0.81250-3.72599-0.35938.h5\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.4939 - categorical_accuracy: 0.8125 - val_loss: 3.7260 - val_categorical_accuracy: 0.3594 - lr: 8.0000e-05\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5039 - categorical_accuracy: 0.8199\n",
      "Epoch 00017: saving model to model_init_2025-03-0506_39_47.413366/model-00017-0.50391-0.81994-3.31411-0.35156.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.5039 - categorical_accuracy: 0.8199 - val_loss: 3.3141 - val_categorical_accuracy: 0.3516 - lr: 8.0000e-05\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5092 - categorical_accuracy: 0.8036\n",
      "Epoch 00018: saving model to model_init_2025-03-0506_39_47.413366/model-00018-0.50924-0.80357-3.31180-0.36719.h5\n",
      "21/21 [==============================] - 108s 5s/step - loss: 0.5092 - categorical_accuracy: 0.8036 - val_loss: 3.3118 - val_categorical_accuracy: 0.3672 - lr: 1.6000e-05\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4595 - categorical_accuracy: 0.8363\n",
      "Epoch 00019: saving model to model_init_2025-03-0506_39_47.413366/model-00019-0.45954-0.83631-2.51717-0.39844.h5\n",
      "21/21 [==============================] - 116s 6s/step - loss: 0.4595 - categorical_accuracy: 0.8363 - val_loss: 2.5172 - val_categorical_accuracy: 0.3984 - lr: 1.6000e-05\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4945 - categorical_accuracy: 0.8348\n",
      "Epoch 00020: saving model to model_init_2025-03-0506_39_47.413366/model-00020-0.49451-0.83482-2.53249-0.35938.h5\n",
      "21/21 [==============================] - 112s 6s/step - loss: 0.4945 - categorical_accuracy: 0.8348 - val_loss: 2.5325 - val_categorical_accuracy: 0.3594 - lr: 1.6000e-05\n"
     ]
    }
   ],
   "source": [
    "history8 = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tenth Model - Image Data augmentation with Conv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "\n",
    "def imageauggenerator(source_path, folder_list, batch_size):\n",
    "    print('Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
    "    \n",
    "    # Define augmentation sequence\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5),                     # Horizontal flip\n",
    "        iaa.Affine(rotate=(-10, 10)),        # Random rotation\n",
    "        iaa.Multiply((0.8, 1.2)),            # Random brightness\n",
    "        iaa.CropAndPad(percent=(-0.05, 0.05)) # Random cropping and padding\n",
    "    ])\n",
    "\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t) // batch_size\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            batch_data = np.zeros((batch_size, 13, 80, 80, 3))\n",
    "            batch_labels = np.zeros((batch_size, 5))\n",
    "\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(source_path + '/' + t[folder + (batch * batch_size)].split(';')[0])\n",
    "                \n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image_path = source_path + '/' + t[folder + (batch * batch_size)].strip().split(';')[0] + '/' + imgs[item]\n",
    "                    image = imread(image_path).astype(np.float32)\n",
    "                    image = resize(image, (80, 80)).astype(np.float32)\n",
    "                    \n",
    "                    # Apply augmentation\n",
    "                    image = seq(image=image)\n",
    "                    \n",
    "                    batch_data[folder, idx, :, :, 0] = image[:, :, 0] / 255\n",
    "                    batch_data[folder, idx, :, :, 1] = image[:, :, 1] / 255\n",
    "                    batch_data[folder, idx, :, :, 2] = image[:, :, 2] / 255\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (batch * batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "        if len(t) % batch_size != 0:\n",
    "            remaining_size = len(t) % batch_size\n",
    "            batch_data = np.zeros((remaining_size, 13, 80, 80, 3))\n",
    "            batch_labels = np.zeros((remaining_size, 5))\n",
    "            \n",
    "            for folder in range(remaining_size):\n",
    "                imgs = os.listdir(source_path + '/' + t[folder + (batch * batch_size)].split(';')[0])\n",
    "                \n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image_path = source_path + '/' + t[folder + (batch * batch_size)].strip().split(';')[0] + '/' + imgs[item]\n",
    "                    image = imread(image_path).astype(np.float32)\n",
    "                    image = resize(image, (80, 80)).astype(np.float32)\n",
    "                    \n",
    "                    # Apply augmentation\n",
    "                    image = seq(image=image)\n",
    "                    \n",
    "                    batch_data[folder, idx, :, :, 0] = image[:, :, 0] / 255\n",
    "                    batch_data[folder, idx, :, :, 1] = image[:, :, 1] / 255\n",
    "                    batch_data[folder, idx, :, :, 2] = image[:, :, 2] / 255\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (batch * batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "# Let me know if you want me to tweak the augmentations or add anything else! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 12:07:48.219053: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2025-03-05 12:07:48.219205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14800 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:40:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (3, 3, 3), padding='same',\n",
    "         input_shape=(13,80,80,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = imageauggenerator(train_path, train_doc, batch_size)\n",
    "val_generator = imageauggenerator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',save_freq = 'epoch')\n",
    "\n",
    "LR =ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 13, 80, 80, 32)    2624      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 13, 80, 80, 32)    0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 6, 40, 40, 32)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 6, 40, 40, 64)     16448     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6, 40, 40, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 3, 20, 20, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 3, 20, 20, 128)    65664     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3, 20, 20, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 1, 10, 10, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1638528   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,731,845\n",
      "Trainable params: 1,731,845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'adam'\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=0\n",
    "validation_steps=0\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /home/datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 12:08:33.522577: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - ETA: 0s - loss: 1.6338 - categorical_accuracy: 0.2217Source path =  /home/datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_init_2025-03-0512_07_25.517768/model-00001-1.63378-0.22172-1.59368-0.21000.h5\n",
      "21/21 [==============================] - 90s 4s/step - loss: 1.6338 - categorical_accuracy: 0.2217 - val_loss: 1.5937 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6063 - categorical_accuracy: 0.2021\n",
      "Epoch 00002: saving model to model_init_2025-03-0512_07_25.517768/model-00002-1.60628-0.20211-1.59891-0.35000.h5\n",
      "21/21 [==============================] - 88s 4s/step - loss: 1.6063 - categorical_accuracy: 0.2021 - val_loss: 1.5989 - val_categorical_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5815 - categorical_accuracy: 0.2609\n",
      "Epoch 00003: saving model to model_init_2025-03-0512_07_25.517768/model-00003-1.58148-0.26094-1.53511-0.21000.h5\n",
      "21/21 [==============================] - 86s 4s/step - loss: 1.5815 - categorical_accuracy: 0.2609 - val_loss: 1.5351 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5804 - categorical_accuracy: 0.2504\n",
      "Epoch 00004: saving model to model_init_2025-03-0512_07_25.517768/model-00004-1.58043-0.25038-1.56874-0.27000.h5\n",
      "21/21 [==============================] - 82s 4s/step - loss: 1.5804 - categorical_accuracy: 0.2504 - val_loss: 1.5687 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5395 - categorical_accuracy: 0.2790\n",
      "Epoch 00005: saving model to model_init_2025-03-0512_07_25.517768/model-00005-1.53949-0.27903-1.43353-0.43000.h5\n",
      "21/21 [==============================] - 86s 4s/step - loss: 1.5395 - categorical_accuracy: 0.2790 - val_loss: 1.4335 - val_categorical_accuracy: 0.4300 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4488 - categorical_accuracy: 0.3575\n",
      "Epoch 00007: saving model to model_init_2025-03-0512_07_25.517768/model-00007-1.44876-0.35747-1.43355-0.37000.h5\n",
      "21/21 [==============================] - 92s 5s/step - loss: 1.4488 - categorical_accuracy: 0.3575 - val_loss: 1.4336 - val_categorical_accuracy: 0.3700 - lr: 0.0010\n",
      "Epoch 8/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4304 - categorical_accuracy: 0.3635\n",
      "Epoch 00008: saving model to model_init_2025-03-0512_07_25.517768/model-00008-1.43035-0.36350-1.28813-0.47000.h5\n",
      "21/21 [==============================] - 84s 4s/step - loss: 1.4304 - categorical_accuracy: 0.3635 - val_loss: 1.2881 - val_categorical_accuracy: 0.4700 - lr: 0.0010\n",
      "Epoch 9/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4197 - categorical_accuracy: 0.3379\n",
      "Epoch 00009: saving model to model_init_2025-03-0512_07_25.517768/model-00009-1.41967-0.33786-1.31083-0.49000.h5\n",
      "21/21 [==============================] - 85s 4s/step - loss: 1.4197 - categorical_accuracy: 0.3379 - val_loss: 1.3108 - val_categorical_accuracy: 0.4900 - lr: 0.0010\n",
      "Epoch 10/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3572 - categorical_accuracy: 0.3831\n",
      "Epoch 00010: saving model to model_init_2025-03-0512_07_25.517768/model-00010-1.35722-0.38311-1.25428-0.44000.h5\n",
      "21/21 [==============================] - 87s 4s/step - loss: 1.3572 - categorical_accuracy: 0.3831 - val_loss: 1.2543 - val_categorical_accuracy: 0.4400 - lr: 0.0010\n",
      "Epoch 11/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3111 - categorical_accuracy: 0.4042\n",
      "Epoch 00011: saving model to model_init_2025-03-0512_07_25.517768/model-00011-1.31114-0.40422-1.12428-0.53000.h5\n",
      "21/21 [==============================] - 86s 4s/step - loss: 1.3111 - categorical_accuracy: 0.4042 - val_loss: 1.1243 - val_categorical_accuracy: 0.5300 - lr: 0.0010\n",
      "Epoch 12/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2267 - categorical_accuracy: 0.4630\n",
      "Epoch 00012: saving model to model_init_2025-03-0512_07_25.517768/model-00012-1.22668-0.46305-1.08329-0.58000.h5\n",
      "21/21 [==============================] - 83s 4s/step - loss: 1.2267 - categorical_accuracy: 0.4630 - val_loss: 1.0833 - val_categorical_accuracy: 0.5800 - lr: 0.0010\n",
      "Epoch 13/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2462 - categorical_accuracy: 0.4706\n",
      "Epoch 00013: saving model to model_init_2025-03-0512_07_25.517768/model-00013-1.24617-0.47059-1.22586-0.47000.h5\n",
      "21/21 [==============================] - 85s 4s/step - loss: 1.2462 - categorical_accuracy: 0.4706 - val_loss: 1.2259 - val_categorical_accuracy: 0.4700 - lr: 0.0010\n",
      "Epoch 14/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2381 - categorical_accuracy: 0.4555\n",
      "Epoch 00014: saving model to model_init_2025-03-0512_07_25.517768/model-00014-1.23808-0.45551-1.15080-0.54000.h5\n",
      "21/21 [==============================] - 88s 4s/step - loss: 1.2381 - categorical_accuracy: 0.4555 - val_loss: 1.1508 - val_categorical_accuracy: 0.5400 - lr: 0.0010\n",
      "Epoch 15/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2089 - categorical_accuracy: 0.4932\n",
      "Epoch 00015: saving model to model_init_2025-03-0512_07_25.517768/model-00015-1.20889-0.49321-1.15292-0.52000.h5\n",
      "21/21 [==============================] - 91s 5s/step - loss: 1.2089 - categorical_accuracy: 0.4932 - val_loss: 1.1529 - val_categorical_accuracy: 0.5200 - lr: 0.0010\n",
      "Epoch 16/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1652 - categorical_accuracy: 0.5189\n",
      "Epoch 00016: saving model to model_init_2025-03-0512_07_25.517768/model-00016-1.16524-0.51885-1.11519-0.56000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "21/21 [==============================] - 86s 4s/step - loss: 1.1652 - categorical_accuracy: 0.5189 - val_loss: 1.1152 - val_categorical_accuracy: 0.5600 - lr: 0.0010\n",
      "Epoch 17/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0966 - categorical_accuracy: 0.5551\n",
      "Epoch 00017: saving model to model_init_2025-03-0512_07_25.517768/model-00017-1.09657-0.55505-1.03883-0.55000.h5\n",
      "21/21 [==============================] - 87s 4s/step - loss: 1.0966 - categorical_accuracy: 0.5551 - val_loss: 1.0388 - val_categorical_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 18/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0754 - categorical_accuracy: 0.5309\n",
      "Epoch 00018: saving model to model_init_2025-03-0512_07_25.517768/model-00018-1.07541-0.53092-0.99669-0.62000.h5\n",
      "21/21 [==============================] - 86s 4s/step - loss: 1.0754 - categorical_accuracy: 0.5309 - val_loss: 0.9967 - val_categorical_accuracy: 0.6200 - lr: 2.0000e-04\n",
      "Epoch 19/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0812 - categorical_accuracy: 0.5490\n",
      "Epoch 00019: saving model to model_init_2025-03-0512_07_25.517768/model-00019-1.08122-0.54902-1.02201-0.59000.h5\n",
      "21/21 [==============================] - 87s 4s/step - loss: 1.0812 - categorical_accuracy: 0.5490 - val_loss: 1.0220 - val_categorical_accuracy: 0.5900 - lr: 2.0000e-04\n",
      "Epoch 20/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0488 - categorical_accuracy: 0.5897\n",
      "Epoch 00020: saving model to model_init_2025-03-0512_07_25.517768/model-00020-1.04884-0.58974-1.01033-0.56000.h5\n",
      "21/21 [==============================] - 83s 4s/step - loss: 1.0488 - categorical_accuracy: 0.5897 - val_loss: 1.0103 - val_categorical_accuracy: 0.5600 - lr: 2.0000e-04\n",
      "Epoch 21/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0258 - categorical_accuracy: 0.5671\n",
      "Epoch 00021: saving model to model_init_2025-03-0512_07_25.517768/model-00021-1.02579-0.56712-1.02409-0.60000.h5\n",
      "21/21 [==============================] - 87s 4s/step - loss: 1.0258 - categorical_accuracy: 0.5671 - val_loss: 1.0241 - val_categorical_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 22/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0162 - categorical_accuracy: 0.5566\n",
      "Epoch 00022: saving model to model_init_2025-03-0512_07_25.517768/model-00022-1.01624-0.55656-0.94985-0.60000.h5\n",
      "21/21 [==============================] - 86s 4s/step - loss: 1.0162 - categorical_accuracy: 0.5566 - val_loss: 0.9498 - val_categorical_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 23/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0517 - categorical_accuracy: 0.5158\n",
      "Epoch 00023: saving model to model_init_2025-03-0512_07_25.517768/model-00023-1.05175-0.51584-0.93088-0.65000.h5\n",
      "21/21 [==============================] - 85s 4s/step - loss: 1.0517 - categorical_accuracy: 0.5158 - val_loss: 0.9309 - val_categorical_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 24/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0132 - categorical_accuracy: 0.5551\n",
      "Epoch 00024: saving model to model_init_2025-03-0512_07_25.517768/model-00024-1.01319-0.55505-1.03728-0.54000.h5\n",
      "21/21 [==============================] - 82s 4s/step - loss: 1.0132 - categorical_accuracy: 0.5551 - val_loss: 1.0373 - val_categorical_accuracy: 0.5400 - lr: 2.0000e-04\n",
      "Epoch 25/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0227 - categorical_accuracy: 0.5732\n",
      "Epoch 00025: saving model to model_init_2025-03-0512_07_25.517768/model-00025-1.02265-0.57315-1.00106-0.59000.h5\n",
      "21/21 [==============================] - 83s 4s/step - loss: 1.0227 - categorical_accuracy: 0.5732 - val_loss: 1.0011 - val_categorical_accuracy: 0.5900 - lr: 2.0000e-04\n",
      "Epoch 26/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9917 - categorical_accuracy: 0.5928\n",
      "Epoch 00026: saving model to model_init_2025-03-0512_07_25.517768/model-00026-0.99169-0.59276-1.02917-0.56000.h5\n",
      "21/21 [==============================] - 85s 4s/step - loss: 0.9917 - categorical_accuracy: 0.5928 - val_loss: 1.0292 - val_categorical_accuracy: 0.5600 - lr: 2.0000e-04\n",
      "Epoch 27/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9735 - categorical_accuracy: 0.6048\n",
      "Epoch 00027: saving model to model_init_2025-03-0512_07_25.517768/model-00027-0.97349-0.60483-0.96984-0.63000.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.9735 - categorical_accuracy: 0.6048 - val_loss: 0.9698 - val_categorical_accuracy: 0.6300 - lr: 2.0000e-04\n",
      "Epoch 28/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9583 - categorical_accuracy: 0.6003\n",
      "Epoch 00028: saving model to model_init_2025-03-0512_07_25.517768/model-00028-0.95827-0.60030-0.93450-0.63000.h5\n",
      "21/21 [==============================] - 80s 4s/step - loss: 0.9583 - categorical_accuracy: 0.6003 - val_loss: 0.9345 - val_categorical_accuracy: 0.6300 - lr: 4.0000e-05\n",
      "Epoch 29/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9689 - categorical_accuracy: 0.6214\n",
      "Epoch 00029: saving model to model_init_2025-03-0512_07_25.517768/model-00029-0.96887-0.62142-0.95658-0.62000.h5\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.9689 - categorical_accuracy: 0.6214 - val_loss: 0.9566 - val_categorical_accuracy: 0.6200 - lr: 4.0000e-05\n",
      "Epoch 30/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9812 - categorical_accuracy: 0.5762\n",
      "Epoch 00030: saving model to model_init_2025-03-0512_07_25.517768/model-00030-0.98120-0.57617-0.94098-0.62000.h5\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.9812 - categorical_accuracy: 0.5762 - val_loss: 0.9410 - val_categorical_accuracy: 0.6200 - lr: 4.0000e-05\n",
      "Epoch 31/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9749 - categorical_accuracy: 0.5777\n",
      "Epoch 00031: saving model to model_init_2025-03-0512_07_25.517768/model-00031-0.97487-0.57768-1.03804-0.52000.h5\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "21/21 [==============================] - 86s 4s/step - loss: 0.9749 - categorical_accuracy: 0.5777 - val_loss: 1.0380 - val_categorical_accuracy: 0.5200 - lr: 4.0000e-05\n",
      "Epoch 32/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9781 - categorical_accuracy: 0.6003\n",
      "Epoch 00032: saving model to model_init_2025-03-0512_07_25.517768/model-00032-0.97811-0.60030-0.90494-0.63000.h5\n",
      "21/21 [==============================] - 82s 4s/step - loss: 0.9781 - categorical_accuracy: 0.6003 - val_loss: 0.9049 - val_categorical_accuracy: 0.6300 - lr: 8.0000e-06\n",
      "Epoch 33/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9434 - categorical_accuracy: 0.6033\n",
      "Epoch 00033: saving model to model_init_2025-03-0512_07_25.517768/model-00033-0.94337-0.60332-0.95945-0.58000.h5\n",
      "21/21 [==============================] - 86s 4s/step - loss: 0.9434 - categorical_accuracy: 0.6033 - val_loss: 0.9594 - val_categorical_accuracy: 0.5800 - lr: 8.0000e-06\n",
      "Epoch 34/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9602 - categorical_accuracy: 0.6003\n",
      "Epoch 00034: saving model to model_init_2025-03-0512_07_25.517768/model-00034-0.96020-0.60030-0.99129-0.59000.h5\n",
      "21/21 [==============================] - 90s 4s/step - loss: 0.9602 - categorical_accuracy: 0.6003 - val_loss: 0.9913 - val_categorical_accuracy: 0.5900 - lr: 8.0000e-06\n",
      "Epoch 35/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9720 - categorical_accuracy: 0.5732\n",
      "Epoch 00035: saving model to model_init_2025-03-0512_07_25.517768/model-00035-0.97197-0.57315-0.90002-0.64000.h5\n",
      "21/21 [==============================] - 85s 4s/step - loss: 0.9720 - categorical_accuracy: 0.5732 - val_loss: 0.9000 - val_categorical_accuracy: 0.6400 - lr: 8.0000e-06\n",
      "Epoch 36/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9341 - categorical_accuracy: 0.6078\n",
      "Epoch 00036: saving model to model_init_2025-03-0512_07_25.517768/model-00036-0.93409-0.60784-0.94951-0.60000.h5\n",
      "21/21 [==============================] - 83s 4s/step - loss: 0.9341 - categorical_accuracy: 0.6078 - val_loss: 0.9495 - val_categorical_accuracy: 0.6000 - lr: 8.0000e-06\n",
      "Epoch 37/40\n",
      "12/21 [================>.............] - ETA: 32s - loss: 0.9168 - categorical_accuracy: 0.6354\n",
      "Epoch 00038: saving model to model_init_2025-03-0512_07_25.517768/model-00038-0.95621-0.58371-0.93324-0.61000.h5\n",
      "21/21 [==============================] - 85s 4s/step - loss: 0.9562 - categorical_accuracy: 0.5837 - val_loss: 0.9332 - val_categorical_accuracy: 0.6100 - lr: 8.0000e-06\n",
      "Epoch 39/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9498 - categorical_accuracy: 0.5973\n",
      "Epoch 00039: saving model to model_init_2025-03-0512_07_25.517768/model-00039-0.94978-0.59729-0.94550-0.52000.h5\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "21/21 [==============================] - 85s 4s/step - loss: 0.9498 - categorical_accuracy: 0.5973 - val_loss: 0.9455 - val_categorical_accuracy: 0.5200 - lr: 8.0000e-06\n",
      "Epoch 40/40\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9711 - categorical_accuracy: 0.6184\n",
      "Epoch 00040: saving model to model_init_2025-03-0512_07_25.517768/model-00040-0.97112-0.61840-0.90429-0.64000.h5\n",
      "21/21 [==============================] - 82s 4s/step - loss: 0.9711 - categorical_accuracy: 0.6184 - val_loss: 0.9043 - val_categorical_accuracy: 0.6400 - lr: 1.6000e-06\n"
     ]
    }
   ],
   "source": [
    "num_epochs9=40\n",
    "history9 = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs9, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "Applied Image Data augmentation on Training data and left the validation data as-is.\n",
    "\n",
    "Overall Model performance: The model steadily learned, with both training and validation accuracy improving over time\n",
    "but still had room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
